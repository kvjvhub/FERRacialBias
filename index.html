<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Detecting Facial Expression Recognition Biases in Large Multimodal Foundation Models">
  <meta name="keywords" content="FER, Expression Recognition, LMFM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Bias to Balance: Detecting Facial Expression Recognition Biases in Large Multimodal Foundation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/kvjvhub">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/kvjvhub">
            COMING SOON
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">From Bias to Balance: Detecting Facial Expression Recognition Biases in Large Multimodal Foundation Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jinyi-c/">Kaylee Chhua</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/zhoujinyi-wen/">Zhoujinyi Wen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="vedanthathalia@gmail.com">Vedant Hathalia</a><sup>3</sup>
            </span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Lake Wales High School,</span>
            <span class="author-block"><sup>2</sup>Thomas Jefferson Senior High School,</span>
            <span class="author-block"><sup>3</sup>Bellarmine College Preparatory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- original link below: https://arxiv.org/pdf/2011.12948-->
                <!--put overleaf read only link here!-->
                <a href="https://www.overleaf.com/read/ngsbbfsnfgkb#6e9b96"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper OVERLEAF</span>
                </a>
              </span>
              <span class="link-block">
                <!--arxiv link goes here-->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv UNAVAILABLE</span>
                </a>
              </span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

        <img src="./static/images/datasetheadshot.png">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">datasets used in this project.</span>
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="content-section">
    <div class="text-content">
      <div class="title is-3 has-text-centered" style="padding-bottom: 20px;">
        <b>What is FER?</b>
      </div>
      <div class="subtitle is-size-5 has-text-centered">
        
        <p>Human communication heavily relies on facial expressions as a means to express emotions, intentions, and reactions without the need for verbal cues. 
          Facial expression recognition (FER) technology capitalizes on this natural form of communication through the analysis of visual media and key facial features, 
          such as the eyes, eyebrows, and mouth, to accurately determine an individual's emotional state. FER encompasses three core stages: face detection, feature extraction, 
          and feature expression classification to distinguish between seven major emotions (anger, disgust, fear, happiness, neutrality, sadness, and surprise).</p>
          <img class="dataone" src="./static/images/270.png" alt="270% increase in FER usage over the past four years.">
          
      </div>
    </div>
    <div class="image-content">
        <img class="datatwo" src="./static/images/mimimifinal.png">
        
    </div>
</div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study addresses the racial biases in facial expression recognition (FER) systems within Large Multimodal Foundation Models (LMFMs). 
            Despite advances in deep learning and the availability of diverse datasets, FER systems often exhibit higher error rates for individuals with darker skin tones. 
          </p>
          <p>
            Existing research predominantly focuses on traditional FER models (CNNs, RNNs, ViTs), leaving a gap in understanding racial biases in LMFMs. 
            We benchmark four leading LMFMs: GPT-4o, PaliGemma, Gemini, and CLIP to assess their performance in facial emotion detection across different racial demographics. 
            A linear classifier trained on CLIP embeddings obtains accuracies of <b>95.9% for RADIATE</b>, <b>90.3% for Tarr</b>, and <b>99.5% for Chicago Face</b>. 
          </p>
          <p>
            Furthermore, we identify that Anger is misclassified as Disgust <b>2.1 times</b> more often in Black Females than White Females.
            This study highlights the need for fairer FER systems and establishes a foundation for developing unbiased, accurate FER technologies. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Related Works</h2>

        <div class="content has-text-justified">
          <p>
            <b>Deep Learning Architectures: </b> In recent years, researchers have become interested in Convolutional
            Neural Networks (CNNs) and hybrid models such as CNN-LSTM (Long Short-Term Memory)
            for <a href = "https://www.sciencedirect.com/science/article/pii/S1877050920318019?via%3Dihub">facial emotion recognition</a>. These architectures are effective at extracting features from
            facial images, but often require large amounts of labeled data for training and suffer from overfitting
            when dealing with <a href = "https://ieeexplore.ieee.org/document/9091188">limited datasets</a>. Although LMFMs show potential due to their <a href = "https://arxiv.org/pdf/2304.14108">vast training
              networks</a>, we discover that they struggle to capture subtle nuances in facial expressions.
          </p>
          <p>
            <b>Dataset Selections: </b> Various factors such as illumination, noise, and blur can impair <a href = "https://ris.utwente.nl/ws/portalfiles/portal/5440996/dutta2012impact.pdf">FER performance</a>. 
            Additionally, upscaling FER datasets to 224 x 224 pixels for neural networks often results in detail loss and <a href = "https://doi.org/10.3390/electronics12183837">reduced classification accuracy</a>. 
            To ensure a fair evaluation of Large Multimodal Foundation Models, we employ high-resolution and uniform datasets.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

</section>

<!--charts section-->

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <img src="./static/images/confusionmatrix.png">
          <h2 class="subtitle has-text-centered">
            <span>Confusion Matrix of Emotion Mistakes Per Gender Per Race</span>
          </h2>
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/verticalheatmaps.png">
          <h2 class="subtitle has-text-centered">
            <span>Percentage of Emotion for Each Race, By Dataset</span>
          </h2>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/confusionmatrix.png">
          <h2 class="subtitle has-text-centered">
            <span>Confusion Matrix of Emotion Mistakes Per Gender Per Race</span>
          </h2>
        </div>
        <div class="item item-fullbody">
          <img src="./static/images/confusionmatrix.png">
          <h2 class="subtitle has-text-centered">
            <span>Confusion Matrix of Emotion Mistakes Per Gender Per Race</span>
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<!--
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
-->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!--
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
            -->
      <a class="icon-link" href="https://github.com/kvjvhub" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>

    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website was built with the help of <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
